{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COs_-Y1xOKeJ"
   },
   "source": [
    "### Running Your Code in Colab\n",
    "\n",
    "We recommend you develop locally and then use Colab to run your code. You'll need to upload the code to drive.\n",
    "\n",
    "For experiments with a batch size greater than one, use the T4 GPU in order to see a *significant* speedup. This is crucial in order to run the experiments in a reasonable amount of time.\n",
    "\n",
    "Make sure to [download](https://drive.google.com/file/d/1mECKLG3NWH9uwFgAYRIdAKrABbTGkbVq/view?usp=sharing) the full training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0X2yjR1bI5C4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Mount your drive to access files in your Google Drive\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# Mount your drive to access files in your Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRpmUnlJJRyG"
   },
   "outputs": [],
   "source": [
    "# cd into the directory where your code is\n",
    "# e.g. %cd /content/drive/MyDrive/cmu/10-301/hw7/handout\n",
    "%cd /content/drive/MyDrive/<path to your code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your data paths. Change this if the data is in a different location\n",
    "tiny_train_stories = \"data/tiny_train_stories.json\"\n",
    "tiny_valid_stories = \"data/tiny_valid_stories.json\"\n",
    "\n",
    "full_train_stories = \"data/HW7_large_stories/train_stories.json\"\n",
    "full_valid_stories = \"data/HW7_large_stories/valid_stories.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4Whb8pdGOMU"
   },
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ign1vlA-c1lG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SelfAttention Test Case 1...Passed\n",
      ".Testing SelfAttention Test Case 2...Passed\n",
      ".Testing RNN Test Case 1...Passed\n",
      ".Testing RNN Test Case 2...Passed\n",
      ".Testing RNNCell Test Case 1...Passed\n",
      ".Testing RNNCell Test Case 2...Passed\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.009s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python test_rnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iwfiNlt_GNpI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "RNNLanguageModel(\n",
      "  (embeddings): Embedding(1024, 64)\n",
      "  (rnn): RNN(\n",
      "    (cell): RNNCell(\n",
      "      (i2h): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (h2h): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (out): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (attention): SelfAttention(\n",
      "    (query_transform): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (key_transform): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (value_transform): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (output_transform): Linear(in_features=32, out_features=128, bias=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=128, out_features=1024, bias=True)\n",
      ")\n",
      "Number of Parameters:  255584\n",
      "Loading data\n",
      "Finished Loading Dataset\n",
      "Batch: 0 | Sequence Length: 128 | Elapsed time (minutes): 7e-06\n",
      "Batch: 1 | Sequence Length: 128 | Elapsed time (minutes): 0.128612\n",
      "Batch: 2 | Sequence Length: 128 | Elapsed time (minutes): 0.135977\n",
      "Batch: 3 | Sequence Length: 128 | Elapsed time (minutes): 0.143383\n",
      "Batch: 4 | Sequence Length: 128 | Elapsed time (minutes): 0.150753\n",
      "Batch: 5 | Sequence Length: 128 | Elapsed time (minutes): 0.158171\n",
      "Batch: 6 | Sequence Length: 128 | Elapsed time (minutes): 0.165309\n",
      "Batch: 7 | Sequence Length: 128 | Elapsed time (minutes): 0.172438\n",
      "Batch: 8 | Sequence Length: 128 | Elapsed time (minutes): 0.179642\n",
      "Batch: 9 | Sequence Length: 128 | Elapsed time (minutes): 0.187044\n",
      "Batch: 10 | Sequence Length: 128 | Elapsed time (minutes): 0.195461\n",
      "Batch: 11 | Sequence Length: 128 | Elapsed time (minutes): 0.202905\n",
      "Batch: 12 | Sequence Length: 128 | Elapsed time (minutes): 0.210608\n",
      "Batch: 13 | Sequence Length: 128 | Elapsed time (minutes): 0.218198\n",
      "Batch: 14 | Sequence Length: 128 | Elapsed time (minutes): 0.225417\n",
      "Batch: 15 | Sequence Length: 128 | Elapsed time (minutes): 0.232831\n",
      "Train Batch Losses: [6.938135, 6.916347, 6.90596, 6.881025, 6.864865, 6.830445, 6.804223, 6.738743, 6.659554, 6.561727, 6.457716, 6.365847, 6.300767, 6.22516, 6.296426, 6.204843]\n",
      "Train Losses [6.938135, 6.916347, 6.90596, 6.881025, 6.864865, 6.830445, 6.804223, 6.738743, 6.659554, 6.561727, 6.457716, 6.365847, 6.300767, 6.22516, 6.296426, 6.204843]\n",
      "Valid Losses [6.9227, 6.906218, 6.886801, 6.861874, 6.829094, 6.785675, 6.727924, 6.652273, 6.554969, 6.436402, 6.315209, 6.276929, 6.289516, 6.248578, 6.190088, 6.14811]\n",
      "Final Train Loss 6.204843\n",
      "Final Valid Loss 6.14811\n",
      "Time 14.399175882339478\n",
      "Final Train Loss:  6.204843\n",
      "Final Valid Loss:  6.14811\n"
     ]
    }
   ],
   "source": [
    "!python rnn.py --train_data {tiny_train_stories} --val_data {tiny_valid_stories} --embed_dim 64 --hidden_dim 128 --train_losses_out train_loss.txt --val_losses_out valid_loss.txt --metrics_out metrics.txt --dk 32 --dv 32 --num_sequences 128 --batch_size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m test_str \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnce upon a time there was a\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m test_str:\n\u001b[0;32m----> 7\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Test prefix:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ts)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Test output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, completion)\n",
      "File \u001b[0;32m~/miniconda3/envs/10601-hw7/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/10601-HW7/rnn.py:566\u001b[0m, in \u001b[0;36mcomplete\u001b[0;34m(prefix, num_tokens, temperature)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomplete\u001b[39m(prefix: \u001b[38;5;28mstr\u001b[39m, num_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m    554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    Generates text completion from language model given text prefix.\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03m    This function has been implemented for you.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;124;03m        str: Text completion\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m     \u001b[43mlm\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prefix, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lm' is not defined"
     ]
    }
   ],
   "source": [
    "from rnn import *\n",
    "\n",
    "lm = torch.load(\"model.pt\")\n",
    "test_str = [\"Once upon a time there was a\"]\n",
    "\n",
    "\n",
    "def complete():\n",
    "\n",
    "    lm.eval()\n",
    "\n",
    "    input = tokenizer.encode(prefix, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    input = input.to(device)\n",
    "    output = lm.generate(input, max_tokens=num_tokens, temperature=temperature)\n",
    "\n",
    "    return tokenizer.decode(output)\n",
    "\n",
    "\n",
    "for ts in test_str:\n",
    "    completion = complete(ts, num_tokens=64, temperature=0.3)\n",
    "    print(\"  Test prefix:\", ts)\n",
    "    print(\"  Test output:\", completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Y89FUa-7Mru"
   },
   "source": [
    "#### 5.1\n",
    "\n",
    "Uncomment the corresponding `embed_hidden_dims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8msmm2XCKveG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10367.72s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "RNNLanguageModel(\n",
      "  (embeddings): Embedding(1024, 64)\n",
      "  (rnn): RNN(\n",
      "    (cell): RNNCell(\n",
      "      (i2h): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (h2h): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (out): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (attention): SelfAttention(\n",
      "    (query_transform): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (key_transform): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (value_transform): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (output_transform): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=64, out_features=1024, bias=True)\n",
      ")\n",
      "Number of Parameters:  177792\n",
      "Loading data\n",
      "Finished Loading Dataset\n",
      "Batch: 0 | Sequence Length: 128 | Elapsed time (minutes): 8.7e-05\n",
      "Batch: 39 | Sequence Length: 128 | Elapsed time (minutes): 0.476304\n",
      "Batch: 78 | Sequence Length: 128 | Elapsed time (minutes): 0.784366\n",
      "Batch: 117 | Sequence Length: 128 | Elapsed time (minutes): 1.073343\n",
      "Batch: 156 | Sequence Length: 128 | Elapsed time (minutes): 1.350783\n",
      "Batch: 195 | Sequence Length: 128 | Elapsed time (minutes): 1.640286\n",
      "Batch: 234 | Sequence Length: 128 | Elapsed time (minutes): 1.928749\n",
      "Batch: 273 | Sequence Length: 128 | Elapsed time (minutes): 2.218835\n",
      "Batch: 312 | Sequence Length: 128 | Elapsed time (minutes): 2.499354\n",
      "Batch: 351 | Sequence Length: 128 | Elapsed time (minutes): 2.783907\n",
      "Train Batch Losses: [6.937713, 6.310434, 5.956666, 5.77896, 5.681302, 5.5873, 5.535621, 5.48965, 5.567123, 5.566029]\n",
      "Train Losses [6.937713, 6.310434, 5.956666, 5.77896, 5.681302, 5.5873, 5.535621, 5.48965, 5.567123, 5.566029]\n",
      "Valid Losses [6.924664, 6.061528, 5.946215, 5.763769, 5.628922, 5.597856, 5.480908, 5.457266, 5.547416, 5.560842]\n",
      "Final Train Loss 5.566029\n",
      "Final Valid Loss 5.560842\n",
      "Time 184.05525302886963\n",
      "Final Train Loss:  5.566029\n",
      "Final Valid Loss:  5.560842\n"
     ]
    }
   ],
   "source": [
    "embed_hidden_dims = 64\n",
    "# embed_hidden_dims = 128\n",
    "# embed_hidden_dims = 256\n",
    "# embed_hidden_dims = 512\n",
    "!python rnn.py --train_data {full_train_stories} --val_data {full_valid_stories} --embed_dim {embed_hidden_dims} --hidden_dim {embed_hidden_dims} --train_losses_out train_losses.txt --val_losses_out val_losses.txt --metrics_out metrics.txt --dk 128 --dv 128 --num_sequences 50000 --batch_size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQ4fn9YDAmiW"
   },
   "source": [
    "#### 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Vh5lsRTuApFD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5316.02s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "RNNLanguageModel(\n",
      "  (embeddings): Embedding(1024, 128)\n",
      "  (rnn): RNN(\n",
      "    (cell): RNNCell(\n",
      "      (i2h): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (h2h): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (out): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (attention): SelfAttention(\n",
      "    (query_transform): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (key_transform): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (value_transform): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (output_transform): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=128, out_features=1024, bias=True)\n",
      ")\n",
      "Number of Parameters:  378752\n",
      "Loading data\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jpthek9/repos/10601-HW7/rnn.py\", line 724, in <module>\n",
      "    main(args)\n",
      "  File \"/Users/jpthek9/repos/10601-HW7/rnn.py\", line 608, in main\n",
      "    train_data = SentenceDataset(args.train_data)\n",
      "  File \"/Users/jpthek9/repos/10601-HW7/rnn.py\", line 19, in __init__\n",
      "    with open(a) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/HW7_large_stories/train_stories.json'\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# batch_size = 64\n",
    "# batch_size = 128\n",
    "# batch_size = 256\n",
    "!python rnn.py --train_data {full_train_stories} --val_data {full_valid_stories} --embed_dim 128 --hidden_dim 128 --train_losses_out train_losses.txt --val_losses_out val_losses.txt --metrics_out metrics.txt --dk 128 --dv 128 --num_sequences 50000 --batch_size {batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3-FuxQlB8TI"
   },
   "source": [
    "#### 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YtltQa_uB9a_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5322.44s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "RNNLanguageModel(\n",
      "  (embeddings): Embedding(1024, 128)\n",
      "  (rnn): RNN(\n",
      "    (cell): RNNCell(\n",
      "      (i2h): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (h2h): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (out): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (attention): SelfAttention(\n",
      "    (query_transform): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (key_transform): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (value_transform): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (output_transform): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=128, out_features=1024, bias=True)\n",
      ")\n",
      "Number of Parameters:  378752\n",
      "Loading data\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jpthek9/repos/10601-HW7/rnn.py\", line 724, in <module>\n",
      "    main(args)\n",
      "  File \"/Users/jpthek9/repos/10601-HW7/rnn.py\", line 608, in main\n",
      "    train_data = SentenceDataset(args.train_data)\n",
      "  File \"/Users/jpthek9/repos/10601-HW7/rnn.py\", line 19, in __init__\n",
      "    with open(a) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/HW7_large_stories/train_stories.json'\n"
     ]
    }
   ],
   "source": [
    "num_sequences = 10000\n",
    "# num_sequences = 20000\n",
    "# num_sequences = 50000\n",
    "# num_sequences = 100000\n",
    "!python rnn.py --train_data {full_train_stories} --val_data {full_valid_stories} --embed_dim 128 --hidden_dim 128 --train_losses_out train_losses.txt --val_losses_out val_losses.txt --metrics_out metrics.txt --dk 128 --dv 128 --num_sequences {num_sequences} --batch_size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iwe2VRiEjO2"
   },
   "source": [
    "#### 5.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtMCKtcJEkZ0"
   },
   "outputs": [],
   "source": [
    "!python rnn.py --train_data {full_train_stories} --val_data {full_valid_stories} --embed_dim 512 --hidden_dim 512 --train_losses_out train_losses.txt --val_losses_out val_losses.txt --metrics_out metrics.txt --dk 256 --dv 256 --num_sequences 250000 --batch_size 128"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "10601-hw7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
